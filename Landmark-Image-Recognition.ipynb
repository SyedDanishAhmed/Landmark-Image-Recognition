{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing and storing the train and test files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the train file and renaming columns\n",
    "train_file_path = '95-891-hw3/HW3_train.csv'\n",
    "train_data = pd.read_csv(train_file_path, header = None)\n",
    "train_data.columns = ['columns_id', 'url', 'landmark_id']\n",
    "\n",
    "# Creating a dictionary for storing image identifier and landmark identifier\n",
    "train_image_names = train_data['columns_id']\n",
    "train_image_labels = train_data['landmark_id']\n",
    "train_image_names_list = train_image_names.to_list()\n",
    "train_image_labels_list = train_image_labels.to_list()\n",
    "train_data_dict = dict(zip(train_image_names_list, train_image_labels_list))\n",
    "\n",
    "# Importing the train file and renaming columns\n",
    "test_file_path = '95-891-hw3/HW3_test.csv'\n",
    "test_data = pd.read_csv(test_file_path, header = None)\n",
    "test_data.columns = ['columns_id', 'url', 'landmark_id']\n",
    "\n",
    "# Creating a dictionary for storing image identifier and landmark identifier\n",
    "test_image_names = test_data['columns_id']\n",
    "test_image_labels = test_data['landmark_id']\n",
    "test_image_names_list = test_image_names.to_list()\n",
    "test_image_labels_list = test_image_labels.to_list()\n",
    "test_data_dict = dict(zip(test_image_names_list, test_image_labels_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identifying and storing the train images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images_folder_path = \"95-891-hw3/images\"\n",
    "train_destination_folder = \"images1/train\"\n",
    "\n",
    "all_files = os.listdir(all_images_folder_path)\n",
    "\n",
    "for image_all in all_files:\n",
    "    for image_train in train_data_dict.keys():\n",
    "        if image_all.strip().startswith(image_train.strip()):\n",
    "            full_path_to_file = all_images_folder_path + '/' + image_all.strip()\n",
    "            train_destination_path = train_destination_folder + '/' + str(train_data_dict[image_train])\n",
    "            shutil.copy(full_path_to_file, train_destination_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identifying and storing the validation images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_destination_folder = \"images1/val\"\n",
    "\n",
    "for image_all in all_files:\n",
    "    for image_test in test_data_dict.keys():\n",
    "        if image_all.strip().startswith(image_test.strip()):\n",
    "            full_path_to_file = all_images_folder_path + '/' + image_all.strip()\n",
    "            test_destination_path = test_destination_folder + '/' + str(test_data_dict[image_test])\n",
    "            shutil.copy(full_path_to_file, test_destination_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the normalizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "data_dir = 'images1'\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=30,\n",
    "                                             shuffle=True, num_workers=4)\n",
    "              for x in ['train', 'val']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function for training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the pretrained resnet18 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv = torchvision.models.resnet18(pretrained=True)\n",
    "for param in model_conv.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_ftrs, len(class_names))\n",
    "\n",
    "model_conv = model_conv.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that only parameters of final layer are being optimized as\n",
    "# opposed to before.\n",
    "optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calling the function to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv = train_model(model_conv, criterion, optimizer_conv,\n",
    "                         exp_lr_scheduler, num_epochs=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the model object to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_conv, \"full_landmark_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the model from the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_full = torch.load('full_landmark_model.pt', map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing the confusion matrix on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[325.,   0.,   3.,   5.,   1.,   0.,   2.,   0.,   1.,   4.],\n",
      "        [  0., 383.,   0.,   0.,   1.,   0.,   0.,   5.,   2.,   1.],\n",
      "        [  2.,   1., 324.,   8.,   8.,   0.,   6.,   3.,   3.,   6.],\n",
      "        [ 13.,   0.,   3., 303.,   3.,   0.,   6.,   3.,   3.,   5.],\n",
      "        [  3.,   1.,   6.,   1., 303.,   0.,   6.,  19.,   5.,   1.],\n",
      "        [  0.,   2.,   2.,   0.,   1., 327.,   2.,   1.,   1.,   1.],\n",
      "        [  0.,   0.,   1.,   3.,   9.,   0., 276.,   9.,   2.,   9.],\n",
      "        [  0.,   1.,   4.,   0.,  19.,   1.,   9., 275.,   6.,   1.],\n",
      "        [  2.,   3.,   1.,   9.,  26.,   1.,   5.,  15., 268.,   2.],\n",
      "        [  1.,   0.,   1.,   3.,   0.,   2.,   1.,   0.,   1., 236.]])\n"
     ]
    }
   ],
   "source": [
    "nb_classes = 10\n",
    "\n",
    "confusion_matrix = torch.zeros(nb_classes, nb_classes)\n",
    "with torch.no_grad():\n",
    "    for i, (inputs, classes) in enumerate(dataloaders['val']):\n",
    "        inputs = inputs.to(device)\n",
    "        classes = classes.to(device)\n",
    "        outputs = model_full(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        for t, p in zip(classes.view(-1), preds.view(-1)):\n",
    "            confusion_matrix[t.long(), p.long()] += 1\n",
    "\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalized confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9393, 0.0000, 0.0087, 0.0151, 0.0027, 0.0000, 0.0064, 0.0000, 0.0034,\n",
      "         0.0150],\n",
      "        [0.0000, 0.9795, 0.0000, 0.0000, 0.0027, 0.0000, 0.0000, 0.0152, 0.0068,\n",
      "         0.0038],\n",
      "        [0.0058, 0.0026, 0.9391, 0.0241, 0.0216, 0.0000, 0.0192, 0.0091, 0.0103,\n",
      "         0.0226],\n",
      "        [0.0376, 0.0000, 0.0087, 0.9127, 0.0081, 0.0000, 0.0192, 0.0091, 0.0103,\n",
      "         0.0188],\n",
      "        [0.0087, 0.0026, 0.0174, 0.0030, 0.8167, 0.0000, 0.0192, 0.0576, 0.0171,\n",
      "         0.0038],\n",
      "        [0.0000, 0.0051, 0.0058, 0.0000, 0.0027, 0.9879, 0.0064, 0.0030, 0.0034,\n",
      "         0.0038],\n",
      "        [0.0000, 0.0000, 0.0029, 0.0090, 0.0243, 0.0000, 0.8818, 0.0273, 0.0068,\n",
      "         0.0338],\n",
      "        [0.0000, 0.0026, 0.0116, 0.0000, 0.0512, 0.0030, 0.0288, 0.8333, 0.0205,\n",
      "         0.0038],\n",
      "        [0.0058, 0.0077, 0.0029, 0.0271, 0.0701, 0.0030, 0.0160, 0.0455, 0.9178,\n",
      "         0.0075],\n",
      "        [0.0029, 0.0000, 0.0029, 0.0090, 0.0000, 0.0060, 0.0032, 0.0000, 0.0034,\n",
      "         0.8872]])\n"
     ]
    }
   ],
   "source": [
    "normalized_confusion_matrix = confusion_matrix/confusion_matrix.sum(0)\n",
    "print(normalized_confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9104612469673157\n"
     ]
    }
   ],
   "source": [
    "print('Test accuracy:',(confusion_matrix.diag().sum()/confusion_matrix.sum()).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating the precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>landmarks id</th>\n",
       "      <th>precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.939306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.979540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.939130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.912651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.816712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.987915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.881789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.917808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.887218</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   landmarks id  precision\n",
       "0             0   0.939306\n",
       "1             1   0.979540\n",
       "2             2   0.939130\n",
       "3             3   0.912651\n",
       "4             4   0.816712\n",
       "5             5   0.987915\n",
       "6             6   0.881789\n",
       "7             7   0.833333\n",
       "8             8   0.917808\n",
       "9             9   0.887218"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Precision = TP / (TP + FP)\n",
    "precision_list = []\n",
    "for i in range(len(confusion_matrix)):\n",
    "    true_positive = confusion_matrix[i][i]\n",
    "    false_positive = 0\n",
    "    for j in range(len(confusion_matrix)):\n",
    "        if i != j:\n",
    "            false_positive += confusion_matrix[j][i]\n",
    "    precision = true_positive / (true_positive + false_positive)\n",
    "    precision_list.append(precision.tolist())\n",
    "\n",
    "landmarks = list(range(10))\n",
    "precision_df = pd.DataFrame(\n",
    "    {'landmarks id': landmarks,\n",
    "     'precision': precision_list,\n",
    "    })\n",
    "precision_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating the recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>landmarks id</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.953079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.977041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.897507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.893805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.878261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.970326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.893204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.870253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.807229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.963265</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   landmarks id    recall\n",
       "0             0  0.953079\n",
       "1             1  0.977041\n",
       "2             2  0.897507\n",
       "3             3  0.893805\n",
       "4             4  0.878261\n",
       "5             5  0.970326\n",
       "6             6  0.893204\n",
       "7             7  0.870253\n",
       "8             8  0.807229\n",
       "9             9  0.963265"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recall = TP / (TP + FN)\n",
    "recall_list = []\n",
    "for i in range(len(confusion_matrix)):\n",
    "    true_positive = confusion_matrix[i][i]\n",
    "    false_negative = confusion_matrix[i].sum() - confusion_matrix[i][i]\n",
    "    recall = true_positive / (true_positive + false_negative)\n",
    "    recall_list.append(recall.tolist())\n",
    "\n",
    "landmarks = list(range(10))\n",
    "recall_df = pd.DataFrame(\n",
    "    {'landmarks id': landmarks,\n",
    "     'recall': recall_list,\n",
    "    })\n",
    "recall_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the precision and recall values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x29ec35226d8>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pr = pd.DataFrame(\n",
    "    {'precision': precision_list,\n",
    "     'recall': recall_list,\n",
    "    })\n",
    "pr.plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "- https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note\n",
    "- The model was trained on Collab and saved in a file. This model file was then loaded in the notebook on local to get the required metrics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
